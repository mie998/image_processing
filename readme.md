## todo
### 11/28
- tuning_net.py では乱数生成関数を切り出してクラスをすっきりさせた。これを net.py にも適応させて。 
- やはり遅すぎる！ sigmoid 関数中にて for 文を用いているのが原因かもしれない。 for 文を用いない実装を考えるべきだ。
- とりあえず課題2までは終わっている。。。と思うので上記が住んだら課題3に進もう。

### 12/6
- 逆伝播法への理解が浅い。。。ゼロから始めるdeep learningをよんで理解を深めるのが課題。  
参考: https://qiita.com/Ugo-Nama/items/04814a13c9ea84978a4c
- ソフトマックス関数の逆関数も必要だと思うが資料を見る限りいらない？？？　この辺もよくわかっていない。  
参考: https://mathtrain.jp/softmax
- 

### 12/12
レポートは最終以外適当でも良いらしい。やっタネ。まとめて提出しても良いらしいから net.py を手直しするよりは back_propagation をちょっと変えたらいい？
毎回MNISTのデータを一からロードするのはあれなので utils.py にデータをキャッシュする関数を用意しました。
これをもちいれば test_data.npz と train_data.npz が生成されるので np.load で読み込んで、どうぞ。
なお、読み込んだ直後は (600, 28**2) の配列になっているので画像の表示がしたければ reshape していただければ。

### 12/19
どういうわけか課題４で予測を6にしかしてくれない。これはおかしい  
パラメータがおかしいのかバッチじゃないからだめなのかわからない。もしかしたらバッチにしてみるとうまくいくのかもしれない  
save がうまくいっていないらしい。どうやらsaveするときに学習する前のデータを使っていそう。

### 12/20
治ったから精度わるいけどいいや()
疑問点としては学習時には90%は超える精度を出しているのに体感50%ほどしか精度がないということ。なんで？？？

処理が早すぎるらしい。全データがちゃんと使えているか確認しろ。  
-> 解決。各layerに個別に与えていたパラメータを書き直すのを忘れていた。NN.layers['w'] = w1  
exp(x) overflow 対策にデータのスケーリングを考えるべき。
-> やった。早くなったかも。エラーも出ない。完璧だ！

課題Aは 5.1 5.2 5.4 までやった。次から畳み込みに入れると良いね。

### 12/26
- batch_normalization に関する考察。  
この層を入れると学習の立ち上がりがおそくなる。さらに急に学習精度がガクッと落ちてしまったりする。(実験では 4epoch から 5epoch で悪化を測定) 
あとは初期値にロバストになっているかレポートにて確認したい。とりあえず変な挙動はしていないかも？



