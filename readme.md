## todo
### 11/28
- tuning_net.py では乱数生成関数を切り出してクラスをすっきりさせた。これを net.py にも適応させて。 
- やはり遅すぎる！ sigmoid 関数中にて for 文を用いているのが原因かもしれない。 for 文を用いない実装を考えるべきだ。
- とりあえず課題2までは終わっている。。。と思うので上記が住んだら課題3に進もう。

### 12/6
- 逆伝播法への理解が浅い。。。ゼロから始めるdeep learningをよんで理解を深めるのが課題。  
参考: https://qiita.com/Ugo-Nama/items/04814a13c9ea84978a4c
- ソフトマックス関数の逆関数も必要だと思うが資料を見る限りいらない？？？　この辺もよくわかっていない。  
参考: https://mathtrain.jp/softmax

### 12/12
レポートは最終以外適当でも良いらしい。やっタネ。まとめて提出しても良いらしいから net.py を手直しするよりは back_propagation をちょっと変えたらいい？
毎回MNISTのデータを一からロードするのはあれなので utils.py にデータをキャッシュする関数を用意しました。
これをもちいれば test_data.npz と train_data.npz が生成されるので np.load で読み込んで、どうぞ。
なお、読み込んだ直後は (600, 28**2) の配列になっているので画像の表示がしたければ reshape していただければ。

### 12/19
どういうわけか課題４で予測を6にしかしてくれない。これはおかしい  
パラメータがおかしいのかバッチじゃないからだめなのかわからない。もしかしたらバッチにしてみるとうまくいくのかもしれない  
save がうまくいっていないらしい。どうやらsaveするときに学習する前のデータを使っていそう。

### 12/20
治ったから精度わるいけどいいや()
疑問点としては学習時には90%は超える精度を出しているのに体感50%ほどしか精度がないということ。なんで？？？

処理が早すぎるらしい。全データがちゃんと使えているか確認しろ。  
-> 解決。各layerに個別に与えていたパラメータを書き直すのを忘れていた。NN.layers['w'] = w1  
exp(x) overflow 対策にデータのスケーリングを考えるべき。
-> やった。早くなったかも。エラーも出ない。完璧だ！

課題Aは 5.1 5.2 5.4 までやった。次から畳み込みに入れると良いね。

### 12/26
- batch_normalization に関する考察。  
この層を入れると学習の立ち上がりがおそくなる。さらに急に学習精度がガクッと落ちてしまったりする。(実験では 4epoch から 5epoch で悪化を測定) 
あとは初期値にロバストになっているかレポートにて確認したい。とりあえず変な挙動はしていないかも？

- まずはチャンネル対応。とりあえず前までとなにも変わらず (batch_size, imagesize**2 * channel_size) とした。  
なお、これだと学習が全然進んでいない。これがチャンネル対応がこれでは不完全だからなのかそもそも多チャンネルは畳み込み前提なのかは現状わかっていない。

### 1/7
ちょっとだけ触ってみた。
- pooling 層のウィンドウサイズと 畳み込みのストライドは同じにするのが好ましいらしい。

- 今やるべきことは
  - [x] col2im, im2col の実装
  - [ ] conv, pooling 層の実装
  - [ ] 実際に学習できているか確認

#### im2colの実装
大切なのは展開先のcolには一度のフィルター適用で使われる要素が入る。つまりストライドとフィルターサイズによってはcolの要素数は展開前よりも多くなる。  
フィルターは結果的に (フィルターの数)*(入力データの1行の要素数) の行列となる。  
ことを踏まえてcol2im, im2colを実装しよう！  
参考: https://qiita.com/kurumen-b/items/236c6255959a266cefaa  

多分できたので conv, pooling の実装から。

#### conv, poolingの実装
- 畳み込み  
w はフィルター, b はバイアス, 入力時のデータはimg形式(多次元)
forward は im2col してフィルター:w　の形を合わせてdot積とるだけ。
backward は？
行列表現がかなりややこしい。一回組み立ててみてデバッグしながら合わせた方がいいかもしれない。
sigmoid 関数の適用は与えられたvectorがなんであれ要素ごとのexpをとるっぽい。つまりpoolingからの  
sigmoid層の結合は画像データを列ベクトルに直す処理を加える必要性はなさそう。

- pooling
場合わけを行う時点で準電波の時に argmax や x は取っておく必要がありそう。

#### 確認する
random_array_generator_normal を多次元に対応させたい！ => やめた。この関数いる？
全結合層のdot積において次元が合わない... 
affine1層の w の次元がおかしそう。計算し直す。 => いけたっぽい。次はgradientのエラー！  
w の次元が2次元じゃないとoptimizerのところでエラーが出る。どうしよう。  
実行時間がかかりすぎて学習できているのか確認できない...? シンプルなCNNでも時間がかかるのは仕方がないことなのだろうか. 
結構実行時間は短くなるようにしたつもりだけど。

できた！！！けど全く学習が進んでいない！！デバッグ。